\thispagestyle{fancy}
\chapter{Anden itteration}
\label{chp:seconditteration}
Efter at første itteration var færdig blev et nyt sæt af krav til systemet præsenteret. På den baggrund blev systemets design opdateret således at det bedre ville kunne udviddes i fremtiden med flere ændringer af samme slags. 

\section{Design og implementering}
De nye krav dikterede at systemet skulle opsættes ved hjælp af en 

\figur{sd_timer_elapsed}{Sekvensdiagram for ATM.\_ticktimer\_Elapsed kald}{fig:sd_timer_elapsed}{1}

*RegionElements
*Configfile
*IEvents
*EventDetectors

\todo[inline]{Johnny}

\section{Unittest}
\todo[inline]{KRK}
unit hest
\subsection{ATM}
\todo[inline]{KRK}
\subsection{TrackHandler}
\todo[inline]{Johnny}
\subsection{RegionEventDetector}
\todo[inline]{Johnny}
\subsection{EventDetectors}
EventsDectors klasserne er testet med forskellige senarier og har været med til at finde fejl i vores implementering. Så det er test der har givet os et godt udbytte. Vi har dog erkendt at vi burde have implementeret beregningen af afstanden mellem de forskellige regionselemeter i en særskilt klasse således at den kun skal testes et sted. Implementeringen i dag gør det rigtig svært at test selve afstands beregningen og det giver også mange ens unit test da vi skal teste den i 5 events detektor klasser. Ved at have beregningen i en klasse for sig selv kan vi påvirke vores unit test af detektor klasserne med en Stub af beregningsklassen og derved bestemme hvornår der skal generes events.

\subsection{Events}
Event klasserne er godt afgrænsede og de har samme interfaces hvilket gør det let at teste dem. De initialiseres med fakes der benyttes som en Mock og Stub. Mocks hjælper med at teste at det er de rigtige værdier der bliver udskrevet samt at de rigtige funktioner bliver kaldt og kun kaldt det antal gange som det er tiltænkt. Der er ingen grænseværdier der skal teste.

\subsection{Excluded code}
Det kan nogle gange give god mening at ekskludere kode i covers analyse. Hvis det er tilladt ikke at skulle opnå 100 \% coveres er det svært at overskue hvor der ikke testes og er det så 90 eller 60 \% der ok? Vi har den opfattelse at alle linjer kode skal unit testes og hvis der er noget som ikke giver mening at tage med i analysen skal det ekskluderes fra analysen og begrundes hvorfor det er i orden.
WriteOutput klasserne som skriver til konsollen, logfilen eller begge er ekskluderet fra unit test og coveres analysen da de skal testes ved integration test og derved se at der skrives til filen eller i konsollen. 
RegionsElements er også ekskluderet da det er get og set funktioner der kan testes, men dette har vi valgt ikke at gøre.

\section{Integrationstest}
\todo[inline]{Johnny}

%
%\section{Teststrategi}
%Gruppen valgte at enhedsteste hver metode så snart den var blevet skrevet. Dette sikrede for det første, at der let kunne holde en coverage nær 100\%, da der blot blev sørget for at få fuld coverage hver eneste gang en ny metode var oprettet. Dette gav en god sikkerhedsfølelse da man da altid kunne stole på, at enhederne virkede i en individuel sammenhæng, og at der derfor ikke var langt til en fungerende integration.  Det gav samtidig en rigtig god afveksling mellem udvikling og test, så arbejdet ikke blev for monotomt. 
%Da enhederne i bunden af hierakiet var blevet testet, påbegyndte vi integrationstest nedefra og op, og til sidst blev hele systemet testet i en afsluttende "big bang" integration.
%
%\subsection{Enhedstest}
%Alle enhedstests blev så vidt muligt konstrueret ved hjælp af isolation frameworket NSubstitute, og testet i Unit-test frameworket NUnit. 
%NSubstitute hjalp os til hurtigt at skrive gode, isolerede unittests. Ulempen ved at bruge et isolation framework var, at man hurtigt glemte om man kodede til en stub eller en mock, fordi det med frameworket var det samme. Vi kom derfor let til at bruge mange mocktests, selvom nogle testcases måske godt kunne være klaret med en stubtest. 
%Under udviklingen af unittests kom gruppen også let til at fokusere udelukkende på coverage. Vi oplevede flere gange at måtte gå tilbage og revurdere tests, fordi de reelt set ikke testede det vi ønskede, på trods af at de havde fuld coverage. Dette var vigtig læring, da man ellers hurtigt kunne stirre sig blind på coverage. 
%
%\subsection{Integrationstest}
%Den primære strategi for integrationstest var som nævnt tidligere bottom-op. Vi startede med at teste de nedre klasser i hierakiet, for derefter at bevæge os opad. De nederste klasser gav en del udfordringer, da det kan være rigtig svært at teste systemets afhængigheder til styresystemet osv. Dette gjorde det blandt andet svært for os at teste output til konsollen, som praktisk talt er umuligt med mindre man intercepter en stream. Problemet ved intercepte en stream er, at man da vil skulle teste i uendelighed, da man da også skal teste sin interception osv. Output til filer blev testet ved at skrive til en fil, læse filen ind igen, for så at sammenligne om det rent faktisk var blevet tilføjet til filen. Efterhånden som vi bevægede os højere op i hierakiet, blev det lettere for os at lave integrationstests, da vi da bedre selv kunne styre afhængighederne.
%Da der var blevet integrationstestet hele vejen op istedet for blot at gøre brug af en big bang test, havde vi en klar formodning om, at systemet ville virke som forventet, hvilket det også gjorde.
%
%\section{Test som support ved ændrede krav}
%De ændrede krav resulterede i, at en række tests skulle skrives om. Nogle få af vores ændrede cases kunne være undgået med et bedre design, men langt de fleste ville være nødvendige uanset designet. 
%De ændrede krav betød også at en række constructors i vores klasser skulle skrives om. Her oplevede vi virkelig hvordan en setup metode kan være en fordel, da vi her blot skulle ændre ét sted, hvorefter alle vores tests var opdateret til at supportere de nye ændringer.
%De nye ændringer krævede naturligvis også en række nye testcases, hvilket hurtigt blev tilføjet til projektet. 